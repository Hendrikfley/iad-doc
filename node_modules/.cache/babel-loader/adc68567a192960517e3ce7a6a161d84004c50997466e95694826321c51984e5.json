{"ast":null,"code":"import { resolveComponent as _resolveComponent, createVNode as _createVNode, Fragment as _Fragment, openBlock as _openBlock, createElementBlock as _createElementBlock } from \"vue\";\nexport function render(_ctx, _cache, $props, $setup, $data, $options) {\n  const _component_navbar = _resolveComponent(\"navbar\");\n  const _component_page_content = _resolveComponent(\"page-content\");\n  return _openBlock(), _createElementBlock(_Fragment, null, [_createVNode(_component_navbar, {\n    pages: $data.pages,\n    \"active-page\": 0,\n    \"nav-link-click\": index => $data.activePage = index\n  }, null, 8 /* PROPS */, [\"pages\", \"nav-link-click\"]), _createVNode(_component_page_content, {\n    page: $data.pages[$data.activePage],\n    innerHTML: $data.pages[$data.activePage].pageContent\n  }, null, 8 /* PROPS */, [\"page\", \"innerHTML\"])], 64 /* STABLE_FRAGMENT */);\n}","map":{"version":3,"names":["_createElementBlock","_Fragment","_createVNode","_component_navbar","pages","$data","index","activePage","_component_page_content","page","innerHTML","pageContent"],"sources":["/Users/hendrikfley/Nextcloud/coco/html-css/vue-start-spa/src/App.vue"],"sourcesContent":["<template>\n  <navbar\n      :pages=\"pages\"\n      :active-page=\"0\"\n      :nav-link-click=\"(index) => (activePage = index)\"\n  ></navbar>\n  <page-content :page=\"pages[activePage]\" v-html=\"pages[activePage].pageContent\"></page-content>\n</template>\n\n<script>\nimport PageContent from \"../src/components/PageContent.vue\";\nimport Navbar from \"./components/Navbar.vue\";\n\nexport default {\n  components: {\n    PageContent,\n    Navbar,\n  },\n  data() {\n    return {\n      activePage: 0,\n      pages: [\n        {\n          link: {text: \"home\", url: \"../sites/home.html\"},\n          pageTitle: \"Home\",\n          pageContent: '<h1>Interaction Design Documentation</h1>\\n' +\n              '<p><img src=\"./iad_exercise_3/iad-exercise_3-12.jpeg\" alt=\"header image\"></p>\\n' +\n              '<p>This is a documentation of the exercises I did for the course \\'Interaction Design\\' at university. Students were tasked to experiment with different inputs and outputs in order to learn about the nature of designing interactive experiences.</p>\\n' +\n              '<p>The documentation aims to explain the different iterations each artifact went through. In order to keep things interesting to read through, I decided to deliver the documentation as a website thus giving me the possibility to design even more interactions - at least theoretically. In practice, I don\\'t have much experience with building websites let alone setting up a single-page-application (SPA) this tries to be. Therefore, the delivery in itself was also a learning experience and the code behind some interactions and effects is heavily inspired by others, that I tried to (give) credit as much as possible.</p>\\n' +\n              '<p>As for my learning of Vue.js - the framework behind the SPA - credits go to <a href=\"https://www.youtube.com/watch?v=1GNsWa_EZdw\">this</a> tutorial.</p>'\n        },\n        {\n          link: {text: \"double click dummy\", url: \"./public/exercise-1.html\"},\n          pageTitle: \"Exercise 1 - 'Double Click Dummy'\",\n          pageContent: '<h2>Exercise 1 - Double Click Dummy</h2>\\n' +\n              '<p>I was teamed up with Tamara and Mario S.</p>\\n' +\n              '<h3>First steps</h3>\\n' +\n              '<p>For the first exercise we had to build and program an interaction using a microcontroller and some components. We were restricted to only use a button or the touch-GPIO of the microcontroller as an input and a single LED as an output. As our microcontroller we chose the ESP32-module as it had both the possibility to try out a button and a touch-GPIO pin that the Arduino UNO lacks (though a quick google search revealed it is not that complicated to set up a similar functionality with the UNO).</p>\\n' +\n              '<p>After the LED was put in the breadboard the right way around, and we fixed some problems with our IDE not being able to upload code to the ESP32, the example code provided in the course repository worked. We fired up the LED while pressing a button - Hooray!</p>\\n' +\n              '<p><img src=\"./iad-exercise_1/iad-exercise_1-1.jpeg\" alt=\"led\"></p>\\n' +\n              '<h3>Brainstorming</h3>\\n' +\n              '<p>We then proceeded to think about an even more interesting interaction with both the LED and the input and came up with two ideas: use the button, and you have to enter a specific rhythm in order to permanently light the LED or - and this was Tamara\\'s idea - use and experiment with the touch-GPIO, and you have to pet an aluminum animal in order to light up the LED.</p>\\n' +\n              '<p>The later idea was the more promising and interesting one from my perspective as you establish a direct physical connection with your palm as an input. Not only does this feel more natural - one might think of touching and stroking an animal without knowing about the animal being an input. Whereas with a button (even if it is a very basic concept and one might assume universally understandable) you do need some experience with buttons in order to recognize them as an input (Think about how a child would approach both setups!).</p>\\n' +\n              '<p><img src=\"https://shorturl.at/koyEZ\" alt=\"tinfoil tapir\">\\n' +\n              'Source: \\'Tinfoil Tapir\\' by M. Pena (Reptangle on <a href=\"https://www.deviantart.com/reptangle/art/Foil-tapir-244925946\">Deviantart</a>)</p>\\n' +\n              '<p>Proceeding to bring this idea to life though, we encountered some issues. The most prominent was, that as soon as we touched the aluminum foil the value in the Arduino\\'s IDE Serial Monitor that was returned by the built-in touchRead()-function reliably dropped but was not or only just slightly and very unreliably affected by the distance covered between the point on the foil where it connects to the ESP and the point where one touches the foil. This made it easy to recognize a touch but hard to determine whether the hand is moving on the foil (thus stroking the animal) or not, which was the plan initially.</p>\\n' +\n              '<p>Our second issue was the time constraint as we aimed to deliver an interesting interaction by the end of the day. We therefore brought back and modified the first idea: The aluminum foil has to be touched in a specific rhythm in order to light up the LED. But what rhythm should it be?</p>\\n' +\n              '<h3>The double click</h3>\\n' +\n              '<p>When thinking of specific and basic rhythms to begin with, we thought about the famous double click with a computer mouse as a required input to use one of the most common operating systems - Microsoft Windows. I recalled my grandpa having issues with the required fast movement of the index finger and as it was put into our awareness in this course to also think about accessibility and context when developing, we figured that we can easily adapt here.</p>\\n' +\n              '<p><img src=\"./iad-exercise_1/double%20click.jpg\" alt=\"Double click is hard for elderly people\"><a href=\"https://www.youtube.com/watch?v=bxNSCuiPiDs\">Watch on YouTube</a></p>\\n' +\n              '<p>Our goal was now to build the prototype for a \\'Double Click Dummy\\', a device for elderly people having issues with this input method. After a quick double tap on the aluminum foil the LED turns on signalling a successful input and turns off when you double tap a second time.</p>\\n' +\n              '<p>Note: We could have reverted to the button with this idea as it is better in mirroring the haptic feedback one gets when using a computer mouse. On the other hand, we were able to quickly model a mouse cause of the aluminum\\'s feature of being able to be wrapped around other objects. One could also argue that it helps to understand touchpad double-clicking which on some laptops lacks haptic feedback or works without it.</p>\\n' +\n              '<p>We then tried to find a solution to detect two consecutive touches within a specific time frame, which was harder than estimated and we were already running late. So with the help of ChatGPT that introduced us to the concept of a debounce delay we came up with the following code:</p>\\n' +\n              '<pre><code class=\"language-c++\">const int ledPin = 15;\\n' +\n              'const int touchPin = 4;\\n' +\n              'const int threshold = 40;\\n' +\n              'int clickCount = 0;\\n' +\n              'bool ledState = false;\\n' +\n              'unsigned long lastClickTime = 0;\\n' +\n              'const unsigned long debounceDelay = 200; // Zeitfenster zwischen zwei Klicks\\n' +\n              '\\n' +\n              'void setup() {\\n' +\n              '  pinMode(ledPin, OUTPUT);\\n' +\n              '  Serial.begin(9600);\\n' +\n              '}\\n' +\n              '\\n' +\n              'void loop() {\\n' +\n              '  int touchValue = touchRead(touchPin);\\n' +\n              '  Serial.println(touchValue);\\n' +\n              '\\n' +\n              '  if (touchValue &lt; threshold) {\\n' +\n              '    unsigned long currentTime = millis();\\n' +\n              '    if (currentTime - lastClickTime &gt; debounceDelay) {\\n' +\n              '      clickCount++;\\n' +\n              '      lastClickTime = currentTime;\\n' +\n              '      if (clickCount == 2) {\\n' +\n              '        ledState = !ledState; // Toggle LED state\\n' +\n              '        digitalWrite(ledPin, ledState ? HIGH : LOW);\\n' +\n              '        clickCount = 0; // Reset click count\\n' +\n              '      }\\n' +\n              '    }\\n' +\n              '  }\\n' +\n              '}\\n' +\n              '</code></pre>\\n' +\n              '<p>The code worked 80 percent of the time. The times it failed, a double click was detected even though the foil was only touched once, which resulted when touched in the exact time in between the reading operations of the touchRead()-function thus resulting in both values to be below the threshold.</p>\\n' +\n              '<p>As shown below, a continuous touch is recognized as a series of double clicks,</p>\\n' +\n              '<p><img src=\"./iad-exercise_1/iad-exercise-1.gif\" alt=\"double click\"></p>\\n' +\n              '<p>Closing thought: With touch inputs being most prominent in today\\'s culture the double click seems already a bit outdated. I only use it to open an application or folder and do not know of any (web)app that requires this kind of input anymore, besides Google Drive or cloud applications that replicate the Explorer (Windows) / Finder (macOS) behavior. It will be interesting to see how long the double click outlasts and if we have to quickly prototype a device like this to tell our grandchildren about this input.</p>'\n        },\n        {\n          link: {text: \"pls smile\", url: \"../sites/interaction-design.html\",\n          },\n          pageTitle: \"Exercise 2 - 'Pls Smile'\",\n          pageContent: \"Hello ps\",\n        },\n        {\n          link: {text: \"ein hauch von liebe\", url: \"../sites/interaction-design.html\",\n          },\n          pageTitle: \"Exercise 3 - 'Ein Hauch von Liebe'\",\n          pageContent: \"Hello ps\",\n        },\n      ],\n    };\n  },\n};\n</script>\n"],"mappings":";;;;uBAAAA,mBAAA,CAAAC,SAAA,SACEC,YAAA,CAIUC,iBAAA;IAHLC,KAAK,EAAEC,KAAA,CAAAD,KAAK;IACZ,aAAW,EAAE,CAAC;IACd,gBAAc,EAAGE,KAAK,IAAMD,KAAA,CAAAE,UAAU,GAAGD;wDAE9CJ,YAAA,CAA8FM,uBAAA;IAA/EC,IAAI,EAAEJ,KAAA,CAAAD,KAAK,CAACC,KAAA,CAAAE,UAAU;IAAGG,SAAsC,EAA9BL,KAAA,CAAAD,KAAK,CAACC,KAAA,CAAAE,UAAU,EAAEI","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}