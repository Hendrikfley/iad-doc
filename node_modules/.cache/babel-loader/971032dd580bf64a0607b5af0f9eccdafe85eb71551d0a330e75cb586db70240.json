{"ast":null,"code":"import { resolveComponent as _resolveComponent, createVNode as _createVNode, Fragment as _Fragment, openBlock as _openBlock, createElementBlock as _createElementBlock } from \"vue\";\nexport function render(_ctx, _cache, $props, $setup, $data, $options) {\n  const _component_navbar = _resolveComponent(\"navbar\");\n  const _component_page_content = _resolveComponent(\"page-content\");\n  return _openBlock(), _createElementBlock(_Fragment, null, [_createVNode(_component_navbar, {\n    pages: $data.pages,\n    \"active-page\": 0,\n    \"nav-link-click\": index => $data.activePage = index\n  }, null, 8 /* PROPS */, [\"pages\", \"nav-link-click\"]), _createVNode(_component_page_content, {\n    page: $data.pages[$data.activePage],\n    innerHTML: $data.pages[$data.activePage].pageContent\n  }, null, 8 /* PROPS */, [\"page\", \"innerHTML\"])], 64 /* STABLE_FRAGMENT */);\n}","map":{"version":3,"names":["_createElementBlock","_Fragment","_createVNode","_component_navbar","pages","$data","index","activePage","_component_page_content","page","innerHTML","pageContent"],"sources":["/Users/hendrikfley/Nextcloud/coco/html-css/vue-start-spa/src/App.vue"],"sourcesContent":["<template>\n  <navbar\n      :pages=\"pages\"\n      :active-page=\"0\"\n      :nav-link-click=\"(index) => (activePage = index)\"\n  ></navbar>\n  <page-content :page=\"pages[activePage]\" v-html=\"pages[activePage].pageContent\"></page-content>\n</template>\n\n<script>\nimport PageContent from \"../src/components/PageContent.vue\";\nimport Navbar from \"./components/Navbar.vue\";\n\nexport default {\n  components: {\n    PageContent,\n    Navbar,\n  },\n  data() {\n    return {\n      activePage: 0,\n      pages: [\n        {\n          link: {text: \"home\", url: \"../sites/home.html\"},\n          pageTitle: \"Home\",\n          pageContent: '<h1>Interaction Design Documentation</h1>\\n' +\n              '<p><img src=\"./iad_exercise_3/iad-exercise_3-12.jpeg\" alt=\"header image\"></p>\\n' +\n              '<p>This is a documentation of the exercises I did for the course \\'Interaction Design\\' at university. Students were tasked to experiment with different inputs and outputs in order to learn about the nature of designing interactive experiences.</p>\\n' +\n              '<p>The documentation aims to explain the different iterations each artifact went through. In order to keep things interesting to read through, I decided to deliver the documentation as a website thus giving me the possibility to design even more interactions - at least theoretically. In practice, I don\\'t have much experience with building websites let alone setting up a single-page-application (SPA) this tries to be. Therefore, the delivery in itself was also a learning experience and the code behind some interactions and effects is heavily inspired by others, that I tried to (give) credit as much as possible.</p>\\n' +\n              '<p>As for my learning of Vue.js - the framework behind the SPA - credits go to <a href=\"https://www.youtube.com/watch?v=1GNsWa_EZdw\">this</a> tutorial.</p>'\n        },\n        {\n          link: {text: \"double click dummy\", url: \"./public/exercise-1.html\"},\n          pageTitle: \"Exercise 1 - 'Double Click Dummy'\",\n          pageContent: '<h2>Exercise 1 - Double Click Dummy</h2>\\n' +\n              '<p>I was teamed up with Tamara and Mario S.</p>\\n' +\n              '<h3>First steps</h3>\\n' +\n              '<p>For the first exercise we had to build and program an interaction using a microcontroller and some components. We were restricted to only use a button or the touch-GPIO of the microcontroller as an input and a single LED as an output. As our microcontroller we chose the ESP32-module as it had both the possibility to try out a button and a touch-GPIO pin that the Arduino UNO lacks (though a quick google search revealed it is not that complicated to set up a similar functionality with the UNO).</p>\\n' +\n              '<p>After the LED was put in the breadboard the right way around, and we fixed some problems with our IDE not being able to upload code to the ESP32, the example code provided in the course repository worked. We fired up the LED while pressing a button - Hooray!</p>\\n' +\n              '<p><img src=\"./iad-exercise_1/iad-exercise_1-1.jpeg\" alt=\"led\"></p>\\n' +\n              '<h3>Brainstorming</h3>\\n' +\n              '<p>We then proceeded to think about an even more interesting interaction with both the LED and the input and came up with two ideas: use the button, and you have to enter a specific rhythm in order to permanently light the LED or - and this was Tamara\\'s idea - use and experiment with the touch-GPIO, and you have to pet an aluminum animal in order to light up the LED.</p>\\n' +\n              '<p>The later idea was the more promising and interesting one from my perspective as you establish a direct physical connection with your palm as an input. Not only does this feel more natural - one might think of touching and stroking an animal without knowing about the animal being an input. Whereas with a button (even if it is a very basic concept and one might assume universally understandable) you do need some experience with buttons in order to recognize them as an input (Think about how a child would approach both setups!).</p>\\n' +\n              '<p><img src=\"https://shorturl.at/koyEZ\" alt=\"tinfoil tapir\">\\n' +\n              'Source: \\'Tinfoil Tapir\\' by M. Pena (Reptangle on <a href=\"https://www.deviantart.com/reptangle/art/Foil-tapir-244925946\">Deviantart</a>)</p>\\n' +\n              '<p>Proceeding to bring this idea to life though, we encountered some issues. The most prominent was, that as soon as we touched the aluminum foil the value in the Arduino\\'s IDE Serial Monitor that was returned by the built-in touchRead()-function reliably dropped but was not or only just slightly and very unreliably affected by the distance covered between the point on the foil where it connects to the ESP and the point where one touches the foil. This made it easy to recognize a touch but hard to determine whether the hand is moving on the foil (thus stroking the animal) or not, which was the plan initially.</p>\\n' +\n              '<p>Our second issue was the time constraint as we aimed to deliver an interesting interaction by the end of the day. We therefore brought back and modified the first idea: The aluminum foil has to be touched in a specific rhythm in order to light up the LED. But what rhythm should it be?</p>\\n' +\n              '<h3>The double click</h3>\\n' +\n              '<p>When thinking of specific and basic rhythms to begin with, we thought about the famous double click with a computer mouse as a required input to use one of the most common operating systems - Microsoft Windows. I recalled my grandpa having issues with the required fast movement of the index finger and as it was put into our awareness in this course to also think about accessibility and context when developing, we figured that we can easily adapt here.</p>\\n' +\n              '<p><img src=\"./iad-exercise_1/double%20click.jpg\" alt=\"Double click is hard for elderly people\"><a href=\"https://www.youtube.com/watch?v=bxNSCuiPiDs\">Watch on YouTube</a></p>\\n' +\n              '<p>Our goal was now to build the prototype for a \\'Double Click Dummy\\', a device for elderly people having issues with this input method. After a quick double tap on the aluminum foil the LED turns on signalling a successful input and turns off when you double tap a second time.</p>\\n' +\n              '<p>Note: We could have reverted to the button with this idea as it is better in mirroring the haptic feedback one gets when using a computer mouse. On the other hand, we were able to quickly model a mouse cause of the aluminum\\'s feature of being able to be wrapped around other objects. One could also argue that it helps to understand touchpad double-clicking which on some laptops lacks haptic feedback or works without it.</p>\\n' +\n              '<p>We then tried to find a solution to detect two consecutive touches within a specific time frame, which was harder than estimated and we were already running late. So with the help of ChatGPT that introduced us to the concept of a debounce delay we came up with the following code:</p>\\n' +\n              '<pre><code class=\"language-c++\">const int ledPin = 15;\\n' +\n              'const int touchPin = 4;\\n' +\n              'const int threshold = 40;\\n' +\n              'int clickCount = 0;\\n' +\n              'bool ledState = false;\\n' +\n              'unsigned long lastClickTime = 0;\\n' +\n              'const unsigned long debounceDelay = 200; // Zeitfenster zwischen zwei Klicks\\n' +\n              '\\n' +\n              'void setup() {\\n' +\n              '  pinMode(ledPin, OUTPUT);\\n' +\n              '  Serial.begin(9600);\\n' +\n              '}\\n' +\n              '\\n' +\n              'void loop() {\\n' +\n              '  int touchValue = touchRead(touchPin);\\n' +\n              '  Serial.println(touchValue);\\n' +\n              '\\n' +\n              '  if (touchValue &lt; threshold) {\\n' +\n              '    unsigned long currentTime = millis();\\n' +\n              '    if (currentTime - lastClickTime &gt; debounceDelay) {\\n' +\n              '      clickCount++;\\n' +\n              '      lastClickTime = currentTime;\\n' +\n              '      if (clickCount == 2) {\\n' +\n              '        ledState = !ledState; // Toggle LED state\\n' +\n              '        digitalWrite(ledPin, ledState ? HIGH : LOW);\\n' +\n              '        clickCount = 0; // Reset click count\\n' +\n              '      }\\n' +\n              '    }\\n' +\n              '  }\\n' +\n              '}\\n' +\n              '</code></pre>\\n' +\n              '<p>The code worked 80 percent of the time. The times it failed, a double click was detected even though the foil was only touched once, which resulted when touched in the exact time in between the reading operations of the touchRead()-function thus resulting in both values to be below the threshold.</p>\\n' +\n              '<p>As shown below, a continuous touch is recognized as a series of double clicks,</p>\\n' +\n              '<p><img src=\"./iad-exercise_1/iad-exercise-1.gif\" alt=\"double click\"></p>\\n' +\n              '<p>Closing thought: With touch inputs being most prominent in today\\'s culture the double click seems already a bit outdated. I only use it to open an application or folder and do not know of any (web)app that requires this kind of input anymore, besides Google Drive or cloud applications that replicate the Explorer (Windows) / Finder (macOS) behavior. It will be interesting to see how long the double click outlasts and if we have to quickly prototype a device like this to tell our grandchildren about this input.</p>'\n        },\n        {\n          link: {text: \"pls smile\", url: \"../sites/interaction-design.html\",\n          },\n          pageTitle: \"Exercise 2 - 'Pls Smile'\",\n          pageContent: '<h2>Exercise 2 - Pls Smile</h2>\\n' +\n              '<p>I was teamed up with Annika and Erik.</p>\\n' +\n              '<p><img src=\"./iad-exercise_2/iad-exercise_2-4.jpeg\" alt=\"pyserial\"></p>\\n' +\n              '<p>This time we were allowed to choose an input of our liking, while the output was still restricted to be the LED.</p>\\n' +\n              '<h3>Brainstorming</h3>\\n' +\n              '<p>We started a quick brainstorming session on what our input and output could be. We thought about a smile as an input to a camera and the detection of smoke or alcohol as an input to one of the sensors we were presented with. Possible outputs on the LED were also collected (it could be lit, blinking fast or slow, blinking in a specific pattern, fading in out slow and fast, or it can burn out).</p>\\n' +\n              '<p>A quick Google search revealed that detecting alcohol was not possible with the sensors presented. Detecting smoke on the other hand would have been a possibility. We decided against it due to the simple fact that smoke detection is a well known concept, and we wanted to create \\'something new\\'. With Erik in our team who already worked with a library that takes emotions as an input, the first idea was the most exciting one. We were asked to also come up and explain a narrative, a context in which the artifact may be used.</p>\\n' +\n              '<h3>Send emotions to the Arduino</h3>\\n' +\n              '<p>While Erik got the Python library he used working again, Annika and I tried to (1) understand how the Python code can work together with the Arduino IDE and also (2) come up with a narrative for our \\'smile detection contraption\\'.</p>\\n' +\n              '<p><img src=\"./iad-exercise_2/iad-exercise_2-3.jpeg\" alt=\"brainstorming\"></p>\\n' +\n              '<p>(1) How to get Python and Arduino to cooperate</p>\\n' +\n              '<p>A good starting point is <a href=\"https://projecthub.arduino.cc/ansh2919/serial-communication-between-python-and-arduino-663756\">this</a> tutorial that explains how the library PySerial is able to send data through the Arduino\\'s serial port. In order to do so in the python code you first define the port to which the Arduino is connected. You can then send data over this port. The following is a <strong>mockup code</strong> and does not represent the actual code that Erik got working.</p>\\n' +\n              '<pre><code class=\"language-python\">\\n' +\n              'import serial\\n' +\n              'import time\\n' +\n              '\\n' +\n              'arduino = serial.Serial(port=\\'COM4\\', baudrate=115200, timeout=0.1)\\n' +\n              '\\n' +\n              'class CameraStream:\\n' +\n              '    def get_emotion(self):\\n' +\n              '        return &quot;smile&quot;\\n' +\n              '\\n' +\n              'camera_stream = CameraStream()\\n' +\n              '\\n' +\n              'def detect_smile(camera_input):\\n' +\n              '    emotion = camera_input.get_emotion()\\n' +\n              '    return emotion == &quot;smile&quot;\\n' +\n              '\\n' +\n              'while True:\\n' +\n              '    if detect_smile(camera_stream):\\n' +\n              '        arduino.write(b\\'smile\\')\\n' +\n              '    else:\\n' +\n              '        time.sleep(0.05)\\n' +\n              '</code></pre>\\n' +\n              '<p>The Arduino on the other hand listens to the port:</p>\\n' +\n              '<pre><code class=\"language-c++\">const int ledPin = 15;\\n' +\n              '\\n' +\n              'void setup() {\\n' +\n              '  Serial.begin(115200);\\n' +\n              '  Serial.setTimeout(1);\\n' +\n              '}\\n' +\n              '\\n' +\n              'void  loop() {\\n' +\n              '  if (Serial.available());\\n' +\n              'digitalWrite(ledPin, HIGH);\\n' +\n              '}\\n' +\n              '</code></pre>\\n' +\n              '<p>Our main problem with this setup was that we were not able to monitor the systems\\' status. One would normally use Arduino IDE\\'s \\'Serial Monitor\\' as we did in the first exercise. The problem is that the monitor is usually running on the same serial port that the python code is now sending data to. We could not identify in the short amount of time, which data is arriving at the port but only that in fact there is data arriving, because our code worked out in the last minute before the presentation we did to show the other groups what we did. We tried to fix the code after the presentation. I will summarize the attempts after I explain the narrative we had in mind for the artifact - and the one we actually presented.</p>\\n' +\n              '<h3>The narrative we had in mind</h3>\\n' +\n              '<p><img src=\"https://pbs.twimg.com/media/DsaQBfvVYAADs3y?format=jpg&amp;name=large\" alt=\"sticky notes on mirror\">\\n' +\n              'Source: <a href=\"https://twitter.com/crashwong/status/1064696723639984128/photo/2\">Alyssa Wong on Twitter/X</a></p>\\n' +\n              '<p>(2) Cheer up a little vs. Please smile (&quot;Lach doch mal&quot; und &quot;Bitte lächeln&quot;)</p>\\n' +\n              '<p>Our idea was to have the camera be mounted next to or behind a mirror, so it can detect when the user is watching themselves in it. The LED can then be mounted next to a sticky note that says \\'pls smile :)\\' resembling similar notes on mirrors (\\'You are beautiful!\\', \\'Carpe diem\\' etc.) that people put there to push their self-esteem or keep a good and productive mood. The LED would, in theory, increase this positive affirmation when smiling in the mirror which indeed has a proven positive effect on oneself.</p>\\n' +\n              '<p>However, in the end I was not sure if the artifact would ever be able to replicate this positive affirmation cause the note was in fact not written by the user themselves. It could then be perceived at best only as an invitation to smile or a demand to do so. What if the user is in fact not in the mood to smile? They may deliver their best fake smile in order to get done with the artifact - ruining its purpose. It would take some user testing and observation to see if the intended positivity is achievable. Or - if people feel pressured by the sticky note - we could modify the purpose and the piece can demonstrate that well-meant sentences like &quot;Cheer up a little!&quot; (&quot;Lach doch mal!&quot;) can be invasive.</p>\\n' +\n              '<h2>Final presentation</h2>\\n' +\n              '<p>We did not have the time to replicate the mirror and sticky note idea. However, Annika thankfully tinkered a smiley face that incorporated the LED, and that we could do the presentation with. She might as well explain it\\'s meaning better in her own documentation.</p>\\n' +\n              '<p>In the end we got the prototype working and the LED lit up as soon as the smile is detected in the Python code. Still, some improvements have to be made, as we were not able to turn the LED off again after it has been lit.</p>\\n' +\n              '<p><img src=\"./iad-exercise_2/iad-exercise_2-7.gif\" alt=\"smile detection\"></p>',\n        },\n        {\n          link: {text: \"ein hauch von liebe\", url: \"../sites/interaction-design.html\",\n          },\n          pageTitle: \"Exercise 3 - 'Ein Hauch von Liebe'\",\n          pageContent: \"Hello ps\",\n        },\n      ],\n    };\n  },\n};\n</script>\n"],"mappings":";;;;uBAAAA,mBAAA,CAAAC,SAAA,SACEC,YAAA,CAIUC,iBAAA;IAHLC,KAAK,EAAEC,KAAA,CAAAD,KAAK;IACZ,aAAW,EAAE,CAAC;IACd,gBAAc,EAAGE,KAAK,IAAMD,KAAA,CAAAE,UAAU,GAAGD;wDAE9CJ,YAAA,CAA8FM,uBAAA;IAA/EC,IAAI,EAAEJ,KAAA,CAAAD,KAAK,CAACC,KAAA,CAAAE,UAAU;IAAGG,SAAsC,EAA9BL,KAAA,CAAAD,KAAK,CAACC,KAAA,CAAAE,UAAU,EAAEI","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}